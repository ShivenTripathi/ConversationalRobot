{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make corpus from nltk stuff(won't have to preprocess much)\n",
    "#train glove on that corpus\n",
    "#define functions you need\n",
    "#what do you need bruhh\n",
    "#make that nice co-occurrence matrix\n",
    "#figure out how optimisation works\n",
    "#figure out initiallisation\n",
    "#figure out training shizz\n",
    "#figure out how you exporting yo model and shizz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from keras.layers import Input, Embedding, Dot, Reshape, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package brown to /home/shiven/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "57340"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "data = brown.sents(categories=brown.categories())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "stopwords_ = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "punctuation = ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
    "for sentence in data:\n",
    "    for word in stopwords_:\n",
    "        token=\" \"+word+\" \"\n",
    "        sentence=[item.replace(token,\" \") for item in sentence]\n",
    "    sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[item.lower() for item in sentences[i]]\n",
    "    sentences[i]=[item*(len(item)>2) for item in sentences[i]]\n",
    "    for pun in punctuation:\n",
    "        sentences[i]=[item.replace(pun,\"\") for item in sentences[i]]\n",
    "        sentences[i]=[item for item in sentences[i] if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['daniel', 'personally', 'led', 'the', 'fight', 'for', 'the', 'measure', 'which', 'had', 'watered', 'down', 'considerably', 'since', 'its', 'rejection', 'two', 'previous', 'legislatures', 'public', 'hearing', 'before', 'the', 'house', 'committee', 'revenue', 'and', 'taxation']\n"
    }
   ],
   "source": [
    "print(sentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "vector_dim=10\n",
    "maxlen=20\n",
    "windowSize=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences_padded = pad_sequences(sequences, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['ruths', 'day', 'and', 'until', 'this', 'year', 'the', 'schedule', 'was', '154', 'games']\n[9177, 108, 3, 161, 9, 117, 2, 2968, 5, 1, 1826]\n[9177  108    3  161    9  117    2 2968    5    1 1826    0    0    0\n    0    0    0    0    0    0]\n11\n11\n20\n"
    }
   ],
   "source": [
    "print(sentences[4000])\n",
    "print(sequences[4000])\n",
    "print(sequences_padded[4000])\n",
    "print(len(sentences[4000]))\n",
    "print(len(sequences[4000]))\n",
    "print(len(sequences_padded[4000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "co_matrix = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sequences_padded:\n",
    "    sentence_size=len(sentence)\n",
    "    for i in range(sentence_size):\n",
    "        for distance in range(1,windowSize+1):\n",
    "            if i+distance<sentence_size:\n",
    "                if sentence[i]>sentence[i+distance]:\n",
    "                    first=sentence[i+distance]\n",
    "                    second=sentence[i]\n",
    "                else:\n",
    "                    second=sentence[i+distance]\n",
    "                    first=sentence[i]\n",
    "                co_matrix[first][second]+=1.0/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "first,second,freqs=[],[],[]\n",
    "for first_id in co_matrix.keys():\n",
    "    for second_id in co_matrix[first_id].keys():\n",
    "        freq=co_matrix[first_id][second_id]\n",
    "        first.append(first_id)\n",
    "        second.append(second_id)\n",
    "        freqs.append(freq)\n",
    "        first.append(second_id)\n",
    "        second.append(first_id)\n",
    "        freqs.append(freq)\n",
    "first=np.array(first)\n",
    "second=np.array(second)\n",
    "freqs=np.array(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1306226,)\n(1306226,)\n(1306226,)\n"
    }
   ],
   "source": [
    "print(np.shape(first))\n",
    "print(np.shape(second))\n",
    "print(np.shape(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time for modelling yay!\n",
    "#shamelessly copied loss haha\n",
    "X_MAX = 100\n",
    "a = 3.0 / 4.0\n",
    "def customLoss(y_true,y_pred):\n",
    "    return K.sum(K.pow(K.clip(y_true / X_MAX, 0.0, 1.0), a) * K.square(y_pred - K.log(y_true)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelMake():\n",
    "\n",
    "    input_target = Input((1,))\n",
    "    input_context = Input((1,))\n",
    "\n",
    "    central_embedding = Embedding(vocab_size, vector_dim, input_length=1)\n",
    "    central_bias = Embedding(vocab_size, 1, input_length=1)\n",
    "\n",
    "    context_embedding = Embedding(vocab_size, vector_dim, input_length=1)\n",
    "    context_bias = Embedding(vocab_size, 1, input_length=1)\n",
    "\n",
    "    vector_target = central_embedding(input_target)\n",
    "    vector_context = context_embedding(input_context)\n",
    "\n",
    "    bias_target = central_bias(input_target)\n",
    "    bias_context = context_bias(input_context)\n",
    "\n",
    "    dot_product = Dot(axes=-1)([vector_target, vector_context])\n",
    "    dot_product = Reshape((1, ))(dot_product)\n",
    "    bias_target = Reshape((1,))(bias_target)\n",
    "    bias_context = Reshape((1,))(bias_context)\n",
    "\n",
    "    prediction = Add()([dot_product, bias_target, bias_context])\n",
    "\n",
    "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
    "    model.compile(loss=customLoss, optimizer=Adam())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "batch_size=1024\n",
    "model=modelMake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/1\n1306226/1306226 [==============================] - 9s 7us/step - loss: 0.1008\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f9bbe36dba8>"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "model.fit([first, second], freqs, epochs=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('try_run': conda)",
   "language": "python",
   "name": "python35664bittryruncondab95fb79d8757413d8f8d35d52e41ba00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}