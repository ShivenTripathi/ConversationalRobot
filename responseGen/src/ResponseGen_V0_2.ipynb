{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " ResponseGen V0.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnbpDxEEbah1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCGWEQ2nbmz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijd06kQ0c1ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! apt install unzip\n",
        "# ! unzip cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3924aeEdKNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAI1pE7BdsMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSL-Zguf0kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQ4tSC7igMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !tar -xvf  'download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz' -C 'data'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0avNyyjK2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "066914e7-3bf7-481a-9436-8e06f92505a8"
      },
      "source": [
        "! wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 18:27:19--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3663376519 (3.4G) [application/gzip]\n",
            "Saving to: ‘en.txt.gz’\n",
            "\n",
            "en.txt.gz           100%[===================>]   3.41G  22.0MB/s    in 2m 39s  \n",
            "\n",
            "2020-06-19 18:29:58 (22.0 MB/s) - ‘en.txt.gz’ saved [3663376519/3663376519]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe6dCOd3lH5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b9226c9f-d956-4663-d31a-c3abb47fe69a"
      },
      "source": [
        "! apt install gunzip\n",
        "! gunzip en.txt.gz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package gunzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ygVssBsIDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# % cd .."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNML07_7sLsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget https://github.com/PolyAI-LDN/conversational-datasets/blob/master/opensubtitles/create_data.py"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtQtXTbuvQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCJ7it3AsQMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _should_skip(line, min_length, max_length):\n",
        "    \"\"\"Whether a line should be skipped depending on the length.\"\"\"\n",
        "    return len(line) < min_length or len(line) > max_length"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTDd-xPZtRi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_example(previous_lines, line, file_id):\n",
        "    \"\"\"Creates examples with multi-line context\n",
        "    The examples will include:\n",
        "        file_id: the name of the file where these lines were obtained.\n",
        "        response: the current line text\n",
        "        context: the previous line text\n",
        "        context/0: 2 lines before\n",
        "        context/1: 3 lines before, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    example = {\n",
        "        'file_id': file_id,\n",
        "        'context': previous_lines[-1],\n",
        "        'response': line,\n",
        "    }\n",
        "    example['file_id'] = file_id\n",
        "    #print(file_id)\n",
        "    example['context'] = previous_lines[-1]\n",
        "    #print(previous_lines[-1])\n",
        "    extra_contexts = previous_lines[:-1]\n",
        "    example.update({\n",
        "        'context/{}'.format(i): context\n",
        "        for i, context in enumerate(extra_contexts[::-1])\n",
        "    })\n",
        "\n",
        "    return example"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuwjLu-ft7oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _preprocess_line(line):\n",
        "    #line = line.decode(\"utf-8\")\n",
        "\n",
        "    # Remove the first word if it is followed by colon (speaker names)\n",
        "    # NOTE: this wont work if the speaker's name has more than one word\n",
        "    line = re.sub('(?:^|(?:[.!?]\\\\s))(\\\\w+):', \"\", line)\n",
        "\n",
        "    # Remove anything between brackets (corresponds to acoustic events).\n",
        "    line = re.sub(\"[\\\\[(](.*?)[\\\\])]\", \"\", line)\n",
        "\n",
        "    # Strip blanks hyphens and line breaks\n",
        "    line = line.strip(\" -\\n\")\n",
        "\n",
        "    return line"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yDBRFXPt-OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_examples_from_file(file_name,file_id, min_length=0, max_length=24,\n",
        "                               num_extra_contexts=2):\n",
        "   # _, file_id = path.split(file_name)\n",
        "    #print(file_id,\"#\")\n",
        "    previous_lines = []\n",
        "    for line in open(file_name):\n",
        "        line = _preprocess_line(line)\n",
        "        \n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        should_skip = _should_skip(\n",
        "            line,\n",
        "            min_length=min_length,\n",
        "            max_length=max_length)\n",
        "\n",
        "        if previous_lines:\n",
        "            should_skip |= _should_skip(\n",
        "                previous_lines[-1],\n",
        "                min_length=min_length,\n",
        "                max_length=max_length)\n",
        "\n",
        "            if not should_skip:\n",
        "                yield create_example(previous_lines, line, file_id)\n",
        "\n",
        "        previous_lines.append(line)\n",
        "        if len(previous_lines) > num_extra_contexts + 1:\n",
        "            del previous_lines[0]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2CLNk0GuBKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _features_to_serialized_tf_example(features):\n",
        "    \"\"\"Convert a string dict to a serialized TF example.\n",
        "    The dictionary maps feature names (strings) to feature values (strings).\n",
        "    \"\"\"\n",
        "    #print(\"hello\")\n",
        "    example = tf.train.Example()\n",
        "    for feature_name, feature_value in features.items():\n",
        "        example.features.feature[feature_name].bytes_list.value.append(\n",
        "            feature_value.encode(\"utf-8\"))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "def _shuffle_examples(examples):\n",
        "    examples |= (\"add random key\" >> beam.Map(\n",
        "        lambda example: (uuid.uuid4(), example)))\n",
        "    examples |= (\"group by key\" >> beam.GroupByKey())\n",
        "    examples |= (\"get shuffled values\" >> beam.FlatMap(lambda t: t[1]))\n",
        "    return examples"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmbBa15zfQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# % cd .."
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TyZGv_l0SNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5JqwZK1FSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pwd"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_KqmRJW1nxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ( head -1000000 en.txt ; ) > million.txt"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwbJtrm_uFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=_create_examples_from_file(file_name='en.txt',file_id='en.txt', num_extra_contexts=0)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-CbqGARumek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=[]\n",
        "sent=[]\n",
        "count=0\n",
        "num_examples = 10**1\n",
        "for x in test:\n",
        "  count+=1\n",
        "  if count<num_examples:\n",
        "    alpha.append(x)  \n",
        "    sent.append  \n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW5xa9ny9JhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cf312ad-553a-43d2-b032-b5a48fae62e2"
      },
      "source": [
        "len(alpha)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcHpkF5E9eOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14424885-2035-49f2-9ee1-8566aa52f75f"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"We're not?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlVQgrr_TVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dat=open('en.txt')\n",
        "sents=[]\n",
        "for line in dat:\n",
        "  sents.append(_preprocess_line(line))\n",
        "  if len(sents)>num_examples-1:\n",
        "    break"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFnc_2r6_oGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "665bb88e-af6e-4a13-80b9-3613c1db9d28"
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLw9JQpYxGId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21e27250-807e-40c1-f2e9-831786afb1c7"
      },
      "source": [
        "import itertools\n",
        "import collections\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "SENT_START_TOKEN = \"SENTENCE_START\"\n",
        "SENT_END_TOKEN = \"SENTENCE_END\"\n",
        "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
        "PADDING_TOKEN = \"PADDING\"\n",
        "\n",
        "\n",
        "def tokenize_text(text_lines):\n",
        "    \"\"\"\n",
        "    Split text into sentences, append start and end tokens to each and tokenize\n",
        "    :param text_lines: list of text lines or list of length one containing all text\n",
        "    :return: list of sentences\n",
        "    \"\"\"\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(line.lower()) for line in text_lines])\n",
        "    sentences = [\"{} {} {}\".format(SENT_START_TOKEN, x, SENT_END_TOKEN) for x in sentences]\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    return tokenized_sentences"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAT3Nv1M9vla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_sents=tokenize_text(sents)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upk4tkB0Bb4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_words_mappings(tokenized_sentences, vocabulary_size):\n",
        "    # Using NLTK\n",
        "    # frequence = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # vocab = frequence.most_common(vocabulary_size)\n",
        "\n",
        "    # Using basic counter\n",
        "    counter = collections.Counter(itertools.chain(*tokenized_sentences))\n",
        "    vocab = counter.most_common(vocabulary_size)\n",
        "    index_to_word = [x[0] for x in vocab]\n",
        "    # Add padding for index 0\n",
        "    index_to_word.insert(0, PADDING_TOKEN)\n",
        "    # Append unknown token (with index = vocabulary size + 1)\n",
        "    index_to_word.append(UNKNOWN_TOKEN)\n",
        "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
        "    return index_to_word, word_to_index"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDcvECskCAJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size=10**4"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BpBI7RvBkW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2w,w2id=get_words_mappings(t_sents,vocabulary_size=vocabulary_size)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj12CfvhHSqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_sents=[]\n",
        "for sent in t_sents:\n",
        "  sent=[w2id[w] for w in sent]\n",
        "  tok_sents.append(sent)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFVwDq11HeVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a859caec-b1d7-418e-c1de-29bd781704b4"
      },
      "source": [
        "print(len(tok_sents))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rR66vRg7J7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array([tok_sents[i] for i in range(0, len(tok_sents)-1)])\n",
        "Y_train = np.array([tok_sents[i] for i in range(1,len(tok_sents))])"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYljerZKITzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4804613a-b6b6-4992-b592-af0bf9eacf5d"
      },
      "source": [
        "print(len(X_train),len(Y_train))\n",
        "print(X_train[0],Y_train[0])"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 9\n",
            "[1, 15, 6, 16, 17, 2] [1, 18, 6, 19, 20, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmkWjjlR4L-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "2fdbfcdb-b639-425d-bdac-2984e35a4690"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 19:23:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-06-19 19:23:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-06-19 19:23:44--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.83MB/s    in 6m 30s  \n",
            "\n",
            "2020-06-19 19:30:14 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aol6KexOQm4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a552838-db10-453d-a4d8-f92b2fb4a16b"
      },
      "source": [
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35AyZok-QpAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "num_words= vocabulary_size\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim)) #create an array of zeros with word_num rows and embedding_dim columns\n",
        "for i, word in enumerate(id2w):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < num_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpJQfxjQsat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import initializers, models, regularizers\n",
        "from keras.layers import Dense, Dropout, Embedding, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Wdvqi9QwdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toyModel = models.Sequential()\n",
        "toyModel.add(Embedding(num_words, \n",
        "                    embedding_dim, \n",
        "                    input_length=24,\n",
        "                      weights=[embedding_matrix],\n",
        "                      trainable=False))"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysrg2DNhRUn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fde178b1-d592-4e89-88be-68d33fd46433"
      },
      "source": [
        "toyModel.summary()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 24, 100)           1000000   \n",
            "=================================================================\n",
            "Total params: 1,000,000\n",
            "Trainable params: 0\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsBl_9wQEVlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}