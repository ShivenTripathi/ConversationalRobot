{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " ResponseGen V0.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnbpDxEEbah1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCGWEQ2nbmz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijd06kQ0c1ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! apt install unzip\n",
        "# ! unzip cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3924aeEdKNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAI1pE7BdsMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSL-Zguf0kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQ4tSC7igMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !tar -xvf  'download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz' -C 'data'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0avNyyjK2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "066914e7-3bf7-481a-9436-8e06f92505a8"
      },
      "source": [
        "! wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 18:27:19--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3663376519 (3.4G) [application/gzip]\n",
            "Saving to: ‘en.txt.gz’\n",
            "\n",
            "en.txt.gz           100%[===================>]   3.41G  22.0MB/s    in 2m 39s  \n",
            "\n",
            "2020-06-19 18:29:58 (22.0 MB/s) - ‘en.txt.gz’ saved [3663376519/3663376519]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe6dCOd3lH5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b9226c9f-d956-4663-d31a-c3abb47fe69a"
      },
      "source": [
        "! apt install gunzip\n",
        "! gunzip en.txt.gz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package gunzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ygVssBsIDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# % cd .."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNML07_7sLsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget https://github.com/PolyAI-LDN/conversational-datasets/blob/master/opensubtitles/create_data.py"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtQtXTbuvQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCJ7it3AsQMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _should_skip(line, min_length, max_length):\n",
        "    \"\"\"Whether a line should be skipped depending on the length.\"\"\"\n",
        "    return len(line) < min_length or len(line) > max_length"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTDd-xPZtRi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_example(previous_lines, line, file_id):\n",
        "    \"\"\"Creates examples with multi-line context\n",
        "    The examples will include:\n",
        "        file_id: the name of the file where these lines were obtained.\n",
        "        response: the current line text\n",
        "        context: the previous line text\n",
        "        context/0: 2 lines before\n",
        "        context/1: 3 lines before, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    example = {\n",
        "        'file_id': file_id,\n",
        "        'context': previous_lines[-1],\n",
        "        'response': line,\n",
        "    }\n",
        "    example['file_id'] = file_id\n",
        "    #print(file_id)\n",
        "    example['context'] = previous_lines[-1]\n",
        "    #print(previous_lines[-1])\n",
        "    extra_contexts = previous_lines[:-1]\n",
        "    example.update({\n",
        "        'context/{}'.format(i): context\n",
        "        for i, context in enumerate(extra_contexts[::-1])\n",
        "    })\n",
        "\n",
        "    return example"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuwjLu-ft7oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _preprocess_line(line):\n",
        "    #line = line.decode(\"utf-8\")\n",
        "\n",
        "    # Remove the first word if it is followed by colon (speaker names)\n",
        "    # NOTE: this wont work if the speaker's name has more than one word\n",
        "    line = re.sub('(?:^|(?:[.!?]\\\\s))(\\\\w+):', \"\", line)\n",
        "\n",
        "    # Remove anything between brackets (corresponds to acoustic events).\n",
        "    line = re.sub(\"[\\\\[(](.*?)[\\\\])]\", \"\", line)\n",
        "\n",
        "    # Strip blanks hyphens and line breaks\n",
        "    line = line.strip(\" -\\n\")\n",
        "\n",
        "    return line"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yDBRFXPt-OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_examples_from_file(file_name,file_id, min_length=0, max_length=24,\n",
        "                               num_extra_contexts=2):\n",
        "   # _, file_id = path.split(file_name)\n",
        "    #print(file_id,\"#\")\n",
        "    previous_lines = []\n",
        "    for line in open(file_name):\n",
        "        line = _preprocess_line(line)\n",
        "        \n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        should_skip = _should_skip(\n",
        "            line,\n",
        "            min_length=min_length,\n",
        "            max_length=max_length)\n",
        "\n",
        "        if previous_lines:\n",
        "            should_skip |= _should_skip(\n",
        "                previous_lines[-1],\n",
        "                min_length=min_length,\n",
        "                max_length=max_length)\n",
        "\n",
        "            if not should_skip:\n",
        "                yield create_example(previous_lines, line, file_id)\n",
        "\n",
        "        previous_lines.append(line)\n",
        "        if len(previous_lines) > num_extra_contexts + 1:\n",
        "            del previous_lines[0]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2CLNk0GuBKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _features_to_serialized_tf_example(features):\n",
        "    \"\"\"Convert a string dict to a serialized TF example.\n",
        "    The dictionary maps feature names (strings) to feature values (strings).\n",
        "    \"\"\"\n",
        "    #print(\"hello\")\n",
        "    example = tf.train.Example()\n",
        "    for feature_name, feature_value in features.items():\n",
        "        example.features.feature[feature_name].bytes_list.value.append(\n",
        "            feature_value.encode(\"utf-8\"))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "def _shuffle_examples(examples):\n",
        "    examples |= (\"add random key\" >> beam.Map(\n",
        "        lambda example: (uuid.uuid4(), example)))\n",
        "    examples |= (\"group by key\" >> beam.GroupByKey())\n",
        "    examples |= (\"get shuffled values\" >> beam.FlatMap(lambda t: t[1]))\n",
        "    return examples"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmbBa15zfQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# % cd .."
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TyZGv_l0SNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ls"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5JqwZK1FSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pwd"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_KqmRJW1nxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! ( head -1000000 en.txt ; ) > million.txt"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwbJtrm_uFMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=_create_examples_from_file(file_name='en.txt',file_id='en.txt', num_extra_contexts=0)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-CbqGARumek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=[]\n",
        "sent=[]\n",
        "count=0\n",
        "num_examples = 10**1\n",
        "for x in test:\n",
        "  count+=1\n",
        "  if count<num_examples:\n",
        "    alpha.append(x)  \n",
        "    sent.append  \n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW5xa9ny9JhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cf312ad-553a-43d2-b032-b5a48fae62e2"
      },
      "source": [
        "len(alpha)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcHpkF5E9eOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14424885-2035-49f2-9ee1-8566aa52f75f"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"We're not?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlVQgrr_TVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dat=open('en.txt')\n",
        "sents=[]\n",
        "for line in dat:\n",
        "  sents.append(_preprocess_line(line))\n",
        "  if len(sents)>num_examples-1:\n",
        "    break"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFnc_2r6_oGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "665bb88e-af6e-4a13-80b9-3613c1db9d28"
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLw9JQpYxGId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c09bff6-ae2f-4aa7-b241-38d930fb62ae"
      },
      "source": [
        "import itertools\n",
        "import collections\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "SENT_START_TOKEN = \"SENTENCE_START\"\n",
        "SENT_END_TOKEN = \"SENTENCE_END\"\n",
        "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
        "PADDING_TOKEN = \"PADDING\"\n",
        "\n",
        "\n",
        "def tokenize_text(text_lines):\n",
        "    \"\"\"\n",
        "    Split text into sentences, append start and end tokens to each and tokenize\n",
        "    :param text_lines: list of text lines or list of length one containing all text\n",
        "    :return: list of sentences\n",
        "    \"\"\"\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(line.lower()) for line in text_lines])\n",
        "    sentences = [\"{} {} {}\".format(SENT_START_TOKEN, x, SENT_END_TOKEN) for x in sentences]\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    for i in range(len(tokenized_sentences)):\n",
        "      while(len(tokenized_sentences[i])<24):\n",
        "        tokenized_sentences[i].append(PADDING_TOKEN)\n",
        "    return tokenized_sentences"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAT3Nv1M9vla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_sents=tokenize_text(sents)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4NP5TMdN7aD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "69a9c6de-c25a-4bea-fc0d-29faa544ebdb"
      },
      "source": [
        "print(t_sents[0])"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'presented', 'by', 'im', 'pictures', 'SENTENCE_END', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upk4tkB0Bb4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_words_mappings(tokenized_sentences, vocabulary_size):\n",
        "    # Using NLTK\n",
        "    # frequence = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # vocab = frequence.most_common(vocabulary_size)\n",
        "\n",
        "    # Using basic counter\n",
        "    counter = collections.Counter(itertools.chain(*tokenized_sentences))\n",
        "    vocab = counter.most_common(vocabulary_size)\n",
        "    index_to_word = [x[0] for x in vocab]\n",
        "    # Add padding for index 0\n",
        "    index_to_word.insert(0, PADDING_TOKEN)\n",
        "    # Append unknown token (with index = vocabulary size + 1)\n",
        "    index_to_word.append(UNKNOWN_TOKEN)\n",
        "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
        "    return index_to_word, word_to_index"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDcvECskCAJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size=10**4"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BpBI7RvBkW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2w,w2id=get_words_mappings(t_sents,vocabulary_size=vocabulary_size)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNSym-JCNvIO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6e3949b-1249-485b-f7aa-ea9b1757fef2"
      },
      "source": [
        "id2w[0]"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PADDING'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj12CfvhHSqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_sents=[]\n",
        "for sent in t_sents:\n",
        "  sent=[w2id[w] for w in sent]\n",
        "  tok_sents.append(sent)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFVwDq11HeVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2308658c-b500-46ad-e367-b47887066cd8"
      },
      "source": [
        "print(len(tok_sents))"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rR66vRg7J7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array([tok_sents[i] for i in range(0, len(tok_sents)-1)])\n",
        "Y_train = np.array([tok_sents[i] for i in range(1,len(tok_sents))])"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYljerZKITzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b494cafb-f745-4ecc-8b59-b0b6fe76cbc4"
      },
      "source": [
        "print(len(X_train),len(Y_train))\n",
        "print(X_train[0],Y_train[0])"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 9\n",
            "[ 2 16  7 17 18  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1] [ 2 19  7 20 21  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmkWjjlR4L-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "534b18f9-36c2-4d8b-83d9-9053e1e069dc"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 20:15:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-06-19 20:15:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-06-19 20:15:31--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        2%[                    ]  19.71M  3.55MB/s    eta 4m 7s  ^C\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: [n]\n",
            "error:  invalid response [[n]]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: junk\n",
            "  inflating: junk                    \n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aol6KexOQm4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c57afd2-ad85-47ef-be8a-93ac2d7c880f"
      },
      "source": [
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35AyZok-QpAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "num_words= vocabulary_size\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim)) #create an array of zeros with word_num rows and embedding_dim columns\n",
        "for i, word in enumerate(id2w):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < num_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOpScrQQPd7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = np.expand_dims(Y_train, -1)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpJQfxjQsat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import LSTM, TimeDistributed, RepeatVector, Input"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Wdvqi9QwdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toyModel = models.Sequential()\n",
        "toyModel.add(Embedding(num_words, \n",
        "                    embedding_dim, \n",
        "                    input_length=24,\n",
        "                      weights=[embedding_matrix],\n",
        "                      trainable=False))"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsBl_9wQEVlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toyModel.add(BatchNormalization())\n",
        "#model.add(TimeDistributed(Flatten())) # not needed if proper batch_input_shape specified before\n",
        "toyModel.add(LSTM(256, \n",
        "               return_sequences=True, \n",
        "               stateful=False, # if stateful model, remember to avoid batches shuffling during training\n",
        "              ))#activation='relu')) ??easily getting loss=nan if using RELU\n",
        "toyModel.add(TimeDistributed(Dense(vocabulary_size, activation='softmax')))"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my8KU9LjK2lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toyModel.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam')"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFQxdXElLZ9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "1ba0a4dd-31ad-4c6c-ea17-da9a4c958dcd"
      },
      "source": [
        "toyModel.summary()"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 24, 100)           1000000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 24, 100)           400       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 24, 256)           365568    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 24, 10000)         2570000   \n",
            "=================================================================\n",
            "Total params: 3,935,968\n",
            "Trainable params: 2,935,768\n",
            "Non-trainable params: 1,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s98YwXfvLe5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef607489-aab6-4c39-cb8e-23e73c3a0396"
      },
      "source": [
        "num_epoch = 100\n",
        "toyModel.fit(X_train, \n",
        "          Y_train, \n",
        "          epochs=num_epoch, \n",
        "          batch_size=1, \n",
        "         )"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 1.7354\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.6141\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 1.5632\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 1.4763\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.3780\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.2757\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.2081\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.1447\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 1.0479\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.9829\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.9124\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.8627\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.8141\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.7906\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.7350\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.6894\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.6501\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.6149\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.5886\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.5689\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.5575\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.5161\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.5125\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.4772\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.4763\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.4757\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.4384\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.4188\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.3963\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.3800\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.3679\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.3514\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.3401\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 1s 66ms/step - loss: 0.3337\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.3559\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.3172\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.3537\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.3106\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.2939\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.2759\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.2656\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 0.2544\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.2528\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.2409\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.2353\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.2250\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 0.2207\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.2236\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.2374\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.2243\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.2155\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 1s 67ms/step - loss: 0.1976\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.2051\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1891\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1827\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.1863\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.1793\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.2612\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.2181\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.1913\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 1s 67ms/step - loss: 0.1801\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.1706\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.1586\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1523\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.1478\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.1434\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.1421\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1430\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.1370\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.1619\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1477\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.1325\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.1278\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1257\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.1221\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1204\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1169\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1145\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.1125\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.1102\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.1088\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.1072\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1104\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.1243\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 0.1114\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1098\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.1092\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.1014\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.0991\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 0.1054\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.0971\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 1s 68ms/step - loss: 0.1100\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 0.1298\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 0.1234\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1004\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 0.1023\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 0.0939\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.0909\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.0913\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 0.0868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa06722dd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ruX7g9OLtuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "\n",
        "class TextGenModel:\n",
        "    SENT_START_TOKEN = \"SENTENCE_START\"\n",
        "    SENT_END_TOKEN = \"SENTENCE_END\"\n",
        "    UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
        "    PAD_TOKEN = \"PADDING\"\n",
        "\n",
        "    TYPES = {'delimited': 0,  # characterized by start and end sentence tokens\n",
        "             'continuous': 1,  # no delimiter tokens\n",
        "             }\n",
        "\n",
        "    def __init__(self, model, index_to_word, word_to_index, sent_max_len=30, temperature=1.0,\n",
        "                 use_embeddings=False, model_type=TYPES['delimited']):\n",
        "        self.model = model\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = word_to_index\n",
        "        self.vocabulary_size = len(word_to_index)\n",
        "        self.unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]\n",
        "        self.pad_token_idx = self.word_to_index[self.PAD_TOKEN]\n",
        "\n",
        "        self.start_token_idx = self.word_to_index.get(self.SENT_START_TOKEN, None)\n",
        "        self.end_token_idx = self.word_to_index.get(self.SENT_END_TOKEN, None)\n",
        "\n",
        "        self.sent_max_len = sent_max_len\n",
        "        self.temperature = temperature\n",
        "        self.use_embeddings = use_embeddings\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def get_sentence(self, sent_min_len, seed=None, seed_max_len = 10):\n",
        "        \"\"\"\n",
        "        Returns a sentence generated by the model.\n",
        "        Method will loop on generated sentences until requirements are met.\n",
        "        :param sent_min_len: minimum length acceptable for the sentence\n",
        "        :param seed: seed text to use for the generation task\n",
        "        :param seed_max_len: max length used for the seed (operated pre-truncation)\n",
        "        :return: the generated string\n",
        "        \"\"\"\n",
        "        if seed == '':\n",
        "            seed = None\n",
        "\n",
        "        start_sentence = self._generate_start_sentence(seed_max_len, seed)\n",
        "\n",
        "        sent = None\n",
        "        while not sent:\n",
        "            sent = self._generate_sentence(sent_min_len, start_sentence)\n",
        "        return sent\n",
        "\n",
        "    def _generate_start_sentence(self, max_len, seed=None):\n",
        "        \"\"\"\n",
        "        Generate sentence based on seed text.\n",
        "        Sentence should then be feed to the model for the actual text generation task.\n",
        "        \"\"\"\n",
        "        # if we have some seed text, start with that\n",
        "        if seed:\n",
        "            idx_sentence = self.transform_sentence(seed, max_len)\n",
        "        else:\n",
        "            # if present use start token\n",
        "            if self.start_token_idx:\n",
        "                start_token_idx = self.start_token_idx\n",
        "            # otherwise use random token index\n",
        "            else:\n",
        "                start_token_idx = np.random.randint(self.vocabulary_size)\n",
        "            idx_sentence = np.array([start_token_idx])\n",
        "        # one hot encode sentence\n",
        "        if not self.use_embeddings:\n",
        "            return self.one_hot_encode_sentence(idx_sentence)\n",
        "        else:\n",
        "            return idx_sentence\n",
        "\n",
        "    # TODO improve on prediction, UNKNOWN handling and timeout\n",
        "    def _generate_sentence(self, min_len, seed_sentence):\n",
        "        \"\"\"\n",
        "        Main procedure for sentence generation.\n",
        "        :param min_len: minimum length acceptable for the generated sentence\n",
        "        :param seed_sentence: seed sentence on which to build newly generated text (can be either one-hot or word-index encoded)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # seed_sentence holds all history of words,\n",
        "        # res_sentence is instead the final resulting response sentence\n",
        "        res_sentence = []\n",
        "\n",
        "        # Repeat until stopping criteria are met\n",
        "        while True:\n",
        "            # Index from which we will sample is the one corresponding to index of last\n",
        "            # word in the fed sentence (RNN next predicted word)\n",
        "            current_idx = max(0, len(seed_sentence) - 1)\n",
        "\n",
        "            # Make predictions and sample index based on returned probabilities\n",
        "            words_probs = self._predict_fun(seed_sentence, idx=current_idx)\n",
        "            sampled_index = TextGenModel.sample_from_prediction(words_probs, temperature=self.temperature)\n",
        "\n",
        "            # TODO could try some more sampling before throwing away already done work?\n",
        "            #Skip if sentence is getting too long or we got an unwanted token\n",
        "            if len(res_sentence) >= self.sent_max_len \\\n",
        "                    or sampled_index == self.unknown_token_idx \\\n",
        "                    or sampled_index == self.pad_token_idx:\n",
        "                return None\n",
        "\n",
        "            # Append result to sentences\n",
        "            res_sentence.append(sampled_index)\n",
        "            # append also to seed sentence (just index if with embedding, one-hot encoded if without\n",
        "            if self.use_embeddings:\n",
        "                seed_sentence = np.append(seed_sentence, [sampled_index], axis=0)\n",
        "            else:\n",
        "                seed_sentence = np.append(seed_sentence, [self.one_hot_encode_word(sampled_index)], axis=0)\n",
        "\n",
        "            # Return if we get an end token and sentence is long enough\n",
        "            if len(res_sentence) > min_len and (not self.end_token_idx or\n",
        "                                                        sampled_index == self.end_token_idx):\n",
        "                return res_sentence\n",
        "\n",
        "    def _generate_answer(self, min_len, oh_question):\n",
        "        # Make predictions and sample index based on probs\n",
        "        answer_probs = self._predict_fun(oh_question)[0]\n",
        "\n",
        "        # Repeat until we get an end token\n",
        "        while True:\n",
        "            idx_sentence = []\n",
        "            for words_probs in answer_probs:\n",
        "                sampled_index = self.sample_from_prediction(words_probs, temperature=self.temperature)\n",
        "\n",
        "                idx_sentence.append(sampled_index)\n",
        "\n",
        "                # Skip if sentence is getting too long or we got an unknown token\n",
        "                if sampled_index == self.unknown_token_idx:\n",
        "                    break\n",
        "                # Return if we get an end token and sentence is long enough\n",
        "                if sampled_index == self.end_token_idx and len(idx_sentence) > min_len:\n",
        "                    return idx_sentence\n",
        "\n",
        "    # TODO add padding for models who need fixed size input\n",
        "    def _predict_fun(self, sentence, idx=None):\n",
        "        \"\"\"\n",
        "        Model prediction based on give sentence\n",
        "        :param sentence: sentence to use (one-hot encoded)\n",
        "        :param idx: if specified, return only probabilities for such index\n",
        "        \"\"\"\n",
        "        # Give required shape (sample=1, sent_len, voc_size)\n",
        "        x = np.expand_dims(sentence, 0)\n",
        "        predictions = self.model.predict(x)\n",
        "        if not idx is None:\n",
        "            return predictions[0][idx]\n",
        "        else:\n",
        "            return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_from_prediction(predictions, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Sample an index from given prediction (a probability distribution)\n",
        "        :param predictions:\n",
        "        :param temperature:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        p = np.log(np.asarray(predictions).astype('float64')) / temperature\n",
        "        p = np.exp(p) / np.sum(np.exp(p))\n",
        "        try:\n",
        "            idx = np.argmax(np.random.multinomial(1, p, 1))\n",
        "        except ValueError as e:\n",
        "            logging.debug(e)\n",
        "            return np.random.choice(len(predictions), 1, p=predictions)[0]\n",
        "        return idx\n",
        "\n",
        "    def transform_sentence(self, sentence, max_len):\n",
        "        \"\"\"\n",
        "        Takes a sentence as string and transform it as required by the model\n",
        "        (tokenize, words to index)\n",
        "        \"\"\"\n",
        "        # Tokenize\n",
        "        # TODO use only spacy\n",
        "        words = nltk.word_tokenize(sentence)  # consider adding lower\n",
        "        # Words to index\n",
        "        idx_sentence = [self.word_to_index.get(w, self.unknown_token_idx) for w in words[-max_len:]]\n",
        "        return idx_sentence\n",
        "\n",
        "    def one_hot_encode_sentence(self, sentence):\n",
        "        \"\"\"\n",
        "        Encode each word of the sentence to one-hot representation.\n",
        "        :param sentence: sentence in words indexes representation\n",
        "        \"\"\"\n",
        "        return np.eye(self.vocabulary_size)[sentence]\n",
        "\n",
        "    def one_hot_encode_word(self, word_idx):\n",
        "        \"\"\"\n",
        "        Encode word to one-hot representation.\n",
        "        :param word_idx: word index to encode\n",
        "        \"\"\"\n",
        "        oh_word = np.zeros(self.vocabulary_size)\n",
        "        if word_idx >= self.vocabulary_size:\n",
        "            raise Exception(\"Word index {} is out of range given a vocabulary size of {}\"\n",
        "                            .format(word_idx, self.vocabulary_size))\n",
        "        else:\n",
        "            oh_word[word_idx] = 1.0\n",
        "        return oh_word\n",
        "\n",
        "    def _sample_word_idx_from(self, words_probs):\n",
        "        unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]\n",
        "        sampled_index = unknown_token_idx\n",
        "        # Sample until a known word is found\n",
        "        while sampled_index == unknown_token_idx:\n",
        "            samples = np.random.multinomial(1, words_probs[-1])\n",
        "            sampled_index = np.argmax(samples)\n",
        "        return sampled_index\n",
        "\n",
        "    # prettify raw generated sentence to string\n",
        "    def pretty_print_sentence(self, sentence, text_max_len=None):\n",
        "        # if model of type delimited, remove end token\n",
        "        if self.model_type == self.TYPES['delimited']:\n",
        "            sentence = sentence[:-1]\n",
        "        # if specified, remove unwanted tail text\n",
        "        if text_max_len:\n",
        "            sentence = sentence[:text_max_len]\n",
        "        # convert sentence from word_indexes to string, and tries to fix spacing\n",
        "        words = [self.index_to_word[word_idx].strip() for word_idx in sentence]\n",
        "        words = [w if re.match(r\"[\\,!\\?\\':\\.]+|n'\", w) else ' ' + w for w in\n",
        "                 words]\n",
        "        # return joined words (spaced added before)\n",
        "        return \"\".join(words).strip()"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkel9CorQaP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_epochs=100"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ1VyEeCQWhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=toyModel.predict(X_train)"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i6jDOEiRHy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = np.argmax(pred, axis = -1)\n",
        "output = [id2w[int(i)] for i in pred]"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztqMstelR0jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de72db03-a3bc-4eb6-fad8-b2acdaacf14a"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.', 'two', '.', 'SENTENCE_END', 'i', \"n't\", ',', 'SENTENCE_END', ',']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWoGmvQSR37J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}